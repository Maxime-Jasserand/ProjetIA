{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Pièces jointes",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "WS_Reseaux_de_neurones_convolutifs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ysbb3XvKlF-q",
        "Si4eppndlF-z"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maxime-Jasserand/ProjetIA/blob/main/WS_Reseaux_de_neurones_convolutifs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysbb3XvKlF-q"
      },
      "source": [
        "# Reseaux de Neurones Confolutifs (CNNs)\n",
        "\n",
        "![GPI-CESI.jpg](attachment:GPI-CESI.jpg)\n",
        "\n",
        "|Auteur|Centre|Modification|\n",
        "|---|---|---|\n",
        "|Nassim HADDAM|Nanterre|2020/12/08|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXvF0Tu-lF-s"
      },
      "source": [
        "L'objectif de de ce workshop est de vous faire acquérir les notions de base sur les réseaux de neurones convolutifs (CNNs). Les réseaux de neurones convolutifs (CNNs) constituent une architecture spéciale permettant de faire de l'apprentissage de taches ayant trait à la vision par ordinateur. L'application que vous allez effectuer consiste à classifier des images de fleurs. Le dataset vous est fourni, et vous allez être guidé tout au long du workshop. Vous allez travailler essentiellement avec [`keras`](https://keras.io/api/) qui est une bibliothèque opensource pour faire du deep learning, et qui a été intégré à la version 2 de [`tensorflow`](https://www.tensorflow.org/). \n",
        "\n",
        "# 1. Chargement des données et consitution du jeu de données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxQerXV4lF-s"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97TIZuPslF-t"
      },
      "source": [
        "Nous commençons par télécharger les images que nous allons utiliser pour faire de la classification d'images :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U61LbbmMlF-u"
      },
      "source": [
        "import pathlib\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnzLm_lOlF-u"
      },
      "source": [
        "Pour commencer, nous devons spécifier quelques paramètres pour l'apprentissage:\n",
        "<ul>\n",
        "    <li>La longueur et la largeur des images. </li>\n",
        "    <li>La taille du batch.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI9l957vlF-u"
      },
      "source": [
        "image_h = 180\n",
        "image_w = 180\n",
        "batch_s = 32\n",
        "\n",
        "image_shape = (image_h, image_w, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqL_06AjlF-v"
      },
      "source": [
        "Ensuite, nous devons partager le jeu de données en jeu d'entrainement et en jeu de test. Pour rappel, le jeu d'entrainement servira à entrainer le réseau de neurones, quant au jeu de test il sert à mesurer les performances de votre architecture sur des données qu'il n'a jamais vues auparavant. Le jeu d'entrainement constitue une fraction du jeu de données total (en l'occurrence 80% dans ce travail). Vous utiliserez la fonction [`tf.keras.preprocessing.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory). Cette fonction sert à générer un jeu de données à partir des fichiers d'image dans un répertoire.\n",
        "\n",
        "**Remarque** : L'algorithme d'apprentissage ne doit **jamais** voir le jeu de test ni s'entrainer sur les exemples le constituant, cela biaiserait les résultats obtenus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFAT61XIlF-w",
        "outputId": "312e3049-51c6-47db-86ed-9d942ad0e44b"
      },
      "source": [
        "# Le train_set\n",
        "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  color_mode = \"rgb\",\n",
        "  subset = \"training\",\n",
        "  seed=42,\n",
        "  image_size = (180,180),\n",
        "  labels = \"inferred\",\n",
        "  label_mode = \"categorical\"\n",
        "  #A COMPLETER\n",
        ")\n",
        "# Le test_set\n",
        "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset = \"validation\",\n",
        "  color_mode = \"rgb\",\n",
        "  seed=42,\n",
        "  image_size = (180,180),\n",
        "  labels = \"inferred\",\n",
        "  label_mode = \"categorical\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3670 files belonging to 5 classes.\n",
            "Using 2936 files for training.\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 734 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lL6FjdElF-w"
      },
      "source": [
        "Le sortie précise qu'on a 3670 fichiers au complet dont 2936 appartenant au jeu d'entrainement et 734 au jeu de test. Maintenant que le jeu de données est prêt, il faut explorer/visualiser les données pour avoir une meilleure intuition de ce qui se passe.\n",
        "\n",
        "# 2. Exploration et visualisation des données\n",
        "Commençons, tout d'abord par afficher le nom des classes. On sait qu'on s'intéresse aux fleurs, mais on ne sait meme pas lesquelles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6IsIG_-lF-x",
        "outputId": "7727d4d1-9122-4bf3-9880-5913741825fe"
      },
      "source": [
        "class_names = train_set.class_names\n",
        "\n",
        "print(class_names)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb1pUzeWlF-x"
      },
      "source": [
        "Le résultat et bel et bien affiché. En français, ça donne `['marguerite', 'pissenlit', 'roses', 'tournesols', 'tulipes']`. Affichons ensuite avec [matplotlib.pyplot.subplot](https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.subplot.html) quelques images pour voir à quoi ces classes correspondent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DdRu2NFblF-x",
        "outputId": "7bf1c56b-2b04-4b1b-f49e-76a56d88d6ea"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"plt.figure(figsize=(8, 8))\n",
        "for images, labels in train_set.take(1):\n",
        "    for i in range(9):\n",
        "      plt.subplot(5,5,i+1)\n",
        "      ax = plt.imshow(np.uint8(images[i]))\n",
        "      plt.title(class_names[labels[i]])\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "plt.show()\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'plt.figure(figsize=(8, 8))\\nfor images, labels in train_set.take(1):\\n    for i in range(9):\\n      plt.subplot(5,5,i+1)\\n      ax = plt.imshow(np.uint8(images[i]))\\n      plt.title(class_names[labels[i]])\\n      plt.axis(\"off\")\\n\\nplt.show()'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5hz6Iw3lF-y"
      },
      "source": [
        "On voit bien qu'il y a différents types de fleurs. Il y a, en apparence, un seul type de fleur par image (ce serait compliqué de faire de la classification autrement).\n",
        "\n",
        "Cependant, on note un certain nombre d'éléments qui peuvent compliquer l'interprétation des images. Une image peut représenter une ou plusieurs fleurs du même type, l'angle de la photo n'est pas toujours le même ainsi que la luminosité, la distance de l'objectif aux fleurs etc. L'arrière-plan peut aussi varier énormément d'une image à l'autre. Enfin, on a aussi parfois d'autres objets dans l'image, qu'ils soient présents dans la nature (abeilles) ou parfois non (sacs). Heureusement, nous avons à notre disposition des outils pour traiter de ce genre de problème, nous le verrons plus bas.\n",
        "\n",
        "On voudrait aussi connaître la taille des données, ça pourrait être utile pour gérer les performances du modèle. Dans la cellule ci-dessous, le type de `train_set` est [`BatchDataset`](https://www.tensorflow.org/api_docs/python/tf/raw_ops/BatchDataset), c'est un objet contenant les batchs d'images. On prend un élément de `train_set` et on affiche le premier batch. C'est un tenseur (matrice à $n$ dimensions) de la forme (32, 180, 180, 3). Ce tenseur représente un batch de 32 images de dimensions 180x180x3 (la dernière dimension représente les canaux RVB). La variable `label_batch` est un tenseur de la forme (32,), il contient les labels des images du batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCEPcqTxlF-y",
        "outputId": "ef2aca46-dcbf-4fc1-c65a-48472a411134"
      },
      "source": [
        "print(type(train_set))\n",
        "images, labels =  next(iter(train_set))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
            "(32, 180, 180, 3)\n",
            "(32, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_y2t8oalF-y"
      },
      "source": [
        "Il est maintenant temps de configurer notre environnement pour passer au vif du sujet.\n",
        "\n",
        "# 3. Configuration de l'environnement pour l'entrainement\n",
        "Dans cette partie, vous devrez utiliser les fonctions [`Dataset.cache`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) et [`Dataset.prefetch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) afin de configurer les données pour améliorer les performances de la façon suivante :\n",
        "- `Dataset.cache()` : Cette fonction sert à forcer le maintien des données en cache dans la mémoire. Vu que le réseau de neurones fait plusieurs passes (qu'on nomme _époque_ ou _epoch_ en anglais) sur les données durant l'apprentissage, cette fonction permet de ne pas avoir à recharger les images à chaque fois. \n",
        "- `Dataset.prefetch()` : Cette fonction permet de faire le prétraitement de l'élément courant du jeu de données (par exemple le batch suivant) en même temps que l'entrainement/évaluation du batch courant par le modèle. Dans un environnement multi-processeurs ou multi-cœur, c'est un gain de temps non négligeable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qo1jeNslF-z"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_set = train_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "test_set = test_set.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si4eppndlF-z"
      },
      "source": [
        "# 4. La normalisation\n",
        "Généralement, en machine learning, vous aurez souvent besoin de transformer l'échelle de valeur des attributs de sorte que les attributs prennent des valeurs qui sont dans le même intervalle. Cette transformation aura pour effet de rendre l'apprentissage plus uniforme au niveau des différents attributs et donc de ne pas surestimer l'importance d'un attribut par rapport à un autre (cela peut arriver si par exemple les attributs ont des échelles complètement différentes). Dans ce workshop vous allez appliquer [la normalisation](https://medium.com/@darrenyaoyao.huang/why-we-need-normalization-in-deep-learning-from-batch-normalization-to-group-normalization-d06ea0e59c17) qui consiste juste à diviser l'entrée (la valeur d'une composante RVB d'un pixel) par 255 pour transformer l'entrée dans l'intervalle [0, 1]. Ce traitement se fait à l'aide de la fonction [`layers.experimental.preprocessing.Rescaling`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHvFJ50JlF-z"
      },
      "source": [
        "# 5. Le modèle de réseau de neurones convolutif (CNN)\n",
        "Le CNN est une architecture spécialisée de réseau de neurones qui a été créé afin d’apprendre efficacement les taches de vision par ordinateur. Un exemple de cette architecture est illustré ci-dessous. La particularité de ces réseaux de neurones est l’introduction de couches convolutives au niveau des couches cachées :\n",
        "-\tVous avez vu dans le workshop de traitement d’images que les opérations de convolution servent à extraire divers caractéristiques et informations contenues dans l’image (nous avons vu la détection de [contours]( https://fr.wikipedia.org/wiki/D%C3%A9tection_de_contours), mais il y aussi la détection de [coins]( https://en.wikipedia.org/wiki/Corner_detection), et aussi la détection de [blob]( https://en.wikipedia.org/wiki/Blob_detection)). \n",
        "-\tVous savez aussi depuis le workshop précédent que dans les réseaux de neurones, les neurones des couches profondes se basent sur les neurones des premières couches pour détecter des attributs de plus haut niveau de la donnée en entrée. Pour les images, par exemple, les caractéristiques de bas niveau sont celles mentionnées dans le point précédent (contours, coins, blobs...), et les caractéristiques de haut niveau pourraient être la présence de certaines motifs, textures, formes construites partant des attributs de bas niveau.\n",
        "![convolutional_architecture.png](attachment:convolutional_architecture.png)\n",
        "Les réseaux de neurones convolutifs combinent les deux idées précédentes pour former des réseaux dont certains neurones servent à représenter des informations plus ou moins locales sur les images à travers l’application de l’opération de convolution. Ces neurones (issus de couches dites convolutives) ne sont connectés qu’à des neurones voisins de la couche précédente. La sortie de ces mêmes neurones est obtenue par l’application de filtres convolués sur la couche précédente. Ces filtres représentent les poids de la couche convolutive et servent à décrire des attributs de l’images appris à partir du dataset. Un exemple de convolution au niveau de la couche convolutive vous est donné ci-dessous (et qui est tiré de ce [lien](https://cs231n.github.io/convolutional-networks/)). Pouvez-vous dire pourquoi il est plus intéressant d’opter pour une architecture par réseaux de neurones convolutifs au lieu de garder l’architecture classique pour les taches de vision par ordinateur ?\n",
        "![gif_convolution.gif](attachment:gif_convolution.gif)\n",
        "\n",
        "<em>À COMPLÉTER</em>\n",
        "\n",
        "Après ces discussions théoriques sur les CNNs, passons maintenant au code. Pour coder votre modèle, vous devez tout d'abord créer un modèle vide à l'aide de la fonction [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) de tensorflow. La suite de cette partie vous guidera dans l'implémentation d'un modèle potentiel pour la classification d'image, elle introduira aussi quelques notions sur les réseaux de neurones convolutifs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZOhirsRlF-z"
      },
      "source": [
        "num_classes = 5 # Nombre de classes et donc aussi nombre de neurones dans la dernière couche\n",
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H0mCgtclF-0"
      },
      "source": [
        "Généralement, en machine learning, vous aurez souvent besoin de transformer l'échelle de valeur des attributs de sorte que les attributs prennent des valeurs qui sont dans le même intervalle. Cette transformation aura pour effet de rendre l'apprentissage plus uniforme au niveau des différents attributs et donc de ne pas surestimer l'importance d'un attribut par rapport à un autre (cela peut arriver si par exemple les attributs ont des échelles complètement différentes).\n",
        "\n",
        "> Bloc en retrait\n",
        "\n",
        "\n",
        "\n",
        "Dans ce workshop vous allez appliquer [la normalisation](https://medium.com/@darrenyaoyao.huang/why-we-need-normalization-in-deep-learning-from-batch-normalization-to-group-normalization-d06ea0e59c17) qui consiste juste à diviser l'entrée (la valeur d'une composante RVB d'un pixel) par 255 pour transformer l'entrée dans l'intervalle [0, 1]. Vous utiliserez la couche [`layers.experimental.preprocessing.Rescaling`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling) à cet effet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_E4xWDWlF-0"
      },
      "source": [
        "model.add(keras.Input(\n",
        "      shape = image_shape\n",
        "    )\n",
        ")\n",
        "model.add(layers.experimental.preprocessing.Rescaling(\n",
        "    scale=1./255\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd3j1mqWlF-0"
      },
      "source": [
        "Après cela, vous allez implémenter votre premier **bloc convolutif**. Un bloc convolutif est constitué de deux couches spécifiques aux réseaux de neurones convolutifs : la [couche convolutive](https://fr.wikipedia.org/wiki/R%C3%A9seau_neuronal_convolutif#Couche_de_convolution_(CONV)) (qui a été introduite brièvement), et la [couche de pooling](https://fr.wikipedia.org/wiki/R%C3%A9seau_neuronal_convolutif#Couche_de_pooling_(POOL)). Pour plus de détails sur ces couches au niveau, [ce Notebook](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_2_cnn.ipynb) sera utile. Les architectures gèrent ces couches de manière potentiellement différentes, mais il est courant d'utiliser ces couches de la manière suivante :\n",
        "\n",
        "- La couche convolutive: Elle sert à détecter la présence d'attributs au niveau de l'image en utilisant l'opération de convolution, et ainsi extraire de plus en plus de caractéristiques. L'application de cette couche vise généralement à retourner des attributs de plus haut niveau sur l'image. Elle a pour effet d’augmenter la profondeur de l’image si le nombre de filtres est assez grand. Dans ce workshop, vous devez faire un [padding](https://ayeshmanthaperera.medium.com/what-is-padding-in-cnns-71b21fb0dd7) de sorte que la taille de la sortie reste la même par rapport à l'entrée. Il devrait y avoir 16 filtres de taille `(3,3)` dans cette première couche. N'oubliez pas que la sortie de cette couche doit être passée à une fonction d'activation (la fonction ReLU).\n",
        "\n",
        "- La couche de pooling : Elle sert à compresser la sortie de la couche convolutive en prenant l'information la plus saillante de celle-ci. La sortie de la couche convolutive est divisée en blocs carrés de même taille. Pour chaque bloc la valeur maximale est retenue et les autres sont ignorés, la hauteur et la largeur de la couche sont réduites.\n",
        "\n",
        "Faire passer la sortie d’une couche à un bloc de convolution permet d’un coté de déceler des attributs de plus haut niveau en combinant ceux de la couche précédente, d’un autre côté de réduire le nombre de neurones de façon à prendre les neurones les plus actives et qui sont nécessaires à la tache (grace à la couche de pooling). Vous devez donc ajuter les deux couches en vous référant à la [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/). Précisons qu'on vous demande de faire un pooling qui prend le maximum et que l'entrée consiste en des images 2D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYXDmurglF-0"
      },
      "source": [
        "# Couche convolutive\n",
        "model.add(\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu')\n",
        ")\n",
        "# Couche de pooling\n",
        "model.add(\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), padding='same')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAvX6tMslF-1"
      },
      "source": [
        "Implémentez le reste du réseau. Celui-ci devra contenir les éléments suivants :\n",
        "- Un bloc convolutif similaire au précédent contenant 32 filtres de hauteur et de largeur de 3.\n",
        "- Un bloc convolutif similaire au précédent contenant 64 filtres de hauteur et de largeur de 3.\n",
        "- Une couche contenant la couche précédente aplatie (flatten).\n",
        "- Une couche entièrement connectée de taille 128.\n",
        "- La couche finale complètement connectée (dense) retournant le résultat de la classification.\n",
        "\n",
        "Quelques remarques devront être relevées au sujet de cette architecture, vous pouvez vous appuyer sur le premier schéma de CNN pour suivre les remarques de cette partie :\n",
        "- Les CNNs sont généralement constitués de deux parties, une partie composée de blocs convolutifs, et une partie composée de couches denses. La première partie sert, indirectement, à compresser la taille de l’image pour la remettre en entrée à la deuxième partie qui aura beaucoup moins de neurones que le nombre d’origine de pixels de l’images.\n",
        "- Les couches de la partie de blocs convolutifs gagnent en profondeur et perdent en hauteur et en largeur à mesure qu’on avance vers des couches profondes dans le réseau. Nous avons besoin de cela car le réseau a besoin d’apprendre à reconnaitre des attributs descriptifs de l’image pour faire de la classification. Le nombre d’attributs s’accroit à mesure qu’on avance dans le réseau, ceci signifie que les neurones des couches profondes perdent en informations spatiales mais gagnent en information descriptives sur l’image.\n",
        "Quel est l’intérêt en mémoire et en temps de calcul de l’utilisation des architectures par réseau de neurones convolutifs ? (Intéressez-vous au nombre de paramètres).\n",
        "<em>À COMPLÉTER</em>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJsZEPpIlF-1"
      },
      "source": [
        "model.add(\n",
        "    # Bloc convolutif ou la taille du filtre est de (32, 3)\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\n",
        ")\n",
        "\n",
        "    # Bloc convolutif ou la taille du filtre est de (64, 3)\n",
        "model.add(\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\n",
        ")\n",
        "\n",
        "    # Applatissement de la couche\n",
        "model.add(\n",
        "    tf.keras.layers.Flatten()\n",
        ")\n",
        "\n",
        "    # Couche entièrement connectée (couche dense)\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(128, activation = \"relu\")\n",
        ")\n",
        "\n",
        "    # Couche entièrement connectée retournant le résultat de la classification\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(5, activation = \"softmax\")\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0QHhYEQlF-1"
      },
      "source": [
        "\n",
        "Choisissons donc cette métrique et compilons le modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WiHgQ9rlF-1",
        "outputId": "28eda538-b84b-44c1-a7b3-9d99ea10a449"
      },
      "source": [
        "model.compile(optimizer =  'adam',\n",
        "              loss =  'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_4 (Rescaling)      (None, 180, 180, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 180, 180, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 90, 90, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 90, 90, 32)        4640      \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 90, 90, 64)        18496     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 518400)            0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               66355328  \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 66,379,557\n",
            "Trainable params: 66,379,557\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FohKGV6GlF-1"
      },
      "source": [
        "Finalement, vous avez à préciser l'évolution de l'erreur d'entrainement et la comparer à l'erreur de test. Précisons que l'entrainement requiert le jeu d'entrainement, le jeu de test ainsi que le nombre d'époques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3qbL6-dolF-1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "a715e957-fc4d-42be-fedf-6379a89793e9"
      },
      "source": [
        "epochs=30\n",
        "\n",
        "history = model.fit(\n",
        "    train_set,\n",
        "    batch_size = batch_s,\n",
        "    epochs = 10,\n",
        "    validation_data = test_set\n",
        ")\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "11/92 [==>...........................] - ETA: 8s - loss: 0.0269 - accuracy: 0.9972"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-bc9267c340ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yalx1l6lF-1"
      },
      "source": [
        "Le code de la cellule précédente permet d’entrainer le modèle et de suivre l’évolution de la précision et de la perte sur le jeu de données d’entrainement et de test. Qu’observez-vous au niveau de ces courbes ? Vous y voyez des signes de surentrainement ou de sous-entrainement ? Pourquoi ?\n",
        "<em>À COMPLÉTER</em>\n",
        "\n",
        "0n remarque que l’évolution des courbes est assez lisse, que feriez-vous si les courbes étaient très bruitées ?\n",
        "\n",
        "<em>À COMPLÉTER</em>\n",
        "\n",
        "Il va donc falloir améliorer les performances de notre algorithme d'apprentissage, et notamment gérer ce problème d'apprentissage. Voyons comment on pourrait s'y prendre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2D4cDQIlF-2"
      },
      "source": [
        "# 5. Techniques de régularisation\n",
        "Les techniques de régularisation sont les techniques qui visent à réduire le surapprentissage. L’objectif est d’éviter que le réseau de neurones mémorise le jeu de données en s’ajustant bêtement sur les particularités du jeu d’entrainement. Il s’agit globalement de réduire les degrés de liberté du réseau de neurones. Dans le cadre de ce wokrshop nous verrons deux techniques de régularisation :\n",
        "-\t**L’augmentation des données** : Avoir plus de données est toujours bon pour réduire le surapprentissage, d’où l’augmentation des données. Le principe et de rajouter de nouvelles images en effectuant des transformations sur le jeu d’entrainement d’origine. Ces nouvelles images sont obtenues par des transformation affines, ou généralement par des transformations réalistes qui ne change pas la nature du label affectée à l’image. Cette technique est très efficace car les réseaux de neurones sont très gourmands en données.\n",
        "-\t**La technique de dropout** : Cette technique consiste à désactiver, à chaque traitement, les neurones d’une couche dense du réseau de manière aléatoire. Le dropout dépend d’un paramètre qui représente la probabilité de désactivation des neurones de la couche. Cette probabilité est le la proportion moyenne de neurones actives dans la couche durant les itérations de l’entrainement. Elle permet de réduire la complexité du réseau de neurones pour réduire le surapprentissage.\n",
        "\n",
        "Ces techniques sont utilisés afin de réduire le surapprentissage au niveau des réseaux de neurones. Mais pouvez dire pourquoi l’augmentation des données est utile alors que les données générées sont issues du jeu de données d’entrainement ? Il y aurait vraiment plus d’information contenue dans notre jeu de données que celle qui est déjà présente ? Essayez de prendre un exemple d'image et de réfléchir sur ce qui pourrait se passer si on applique une rotation au niveau de cette image.\n",
        "\n",
        "<em>À COMPLÉTER</em>\n",
        "Pouvez-vous détailler en quoi le dropout est utile pour gérer les problèmes de surapprentissage ? Quels sont les avantages du dopout ? Il faut penser en termes de taille du modèle et de sa capacité à mémoriser/apprendre les détails spécifiques au jeu de données.\n",
        "<em>À COMPLÉTER</em>\n",
        "\n",
        "Dans les cellules qui suivent, vous allez ré-implémenter le modèle et refaire l’entrainement en incluant une couche de dropout (précédent la couche d’aplatissement), le tout en rajoutant une couche d’augmentation de données. Implémentez tout d'abord le réseau précédent en rajoutant seulement le dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIqSArSNwlmN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "p0aJHdTvlF-2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e033aa37-0396-4733-da9b-72d6b5181d4f"
      },
      "source": [
        "# Le modèle\n",
        "model_with_dropout = Sequential()\n",
        "model_with_dropout.add(keras.Input(shape=(180, 180, 3)))  # 180x180 RGB images\n",
        "model_with_dropout.add(layers.Dropout(.2, input_shape=(2,)))\n",
        "model_with_dropout.add(layers.experimental.preprocessing.Rescaling(scale=1./255))\n",
        "model_with_dropout.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", data_format=None))\n",
        "model_with_dropout.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
        "model_with_dropout.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
        "model_with_dropout.add(layers.Flatten())\n",
        "model_with_dropout.add(layers.Dense(128, activation='relu'))\n",
        "model_with_dropout.add(layers.Dropout(0.2))\n",
        "model_with_dropout.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "model_with_dropout.compile(optimizer =  'adam',\n",
        "              loss =  'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_with_dropout.summary()\n",
        "\n",
        "history = model_with_dropout.fit(\n",
        "    train_set,\n",
        "    batch_size = batch_s,\n",
        "    epochs = 10,\n",
        "    validation_data = test_set\n",
        ")\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_6 (Dropout)          (None, 180, 180, 3)       0         \n",
            "_________________________________________________________________\n",
            "rescaling_5 (Rescaling)      (None, 180, 180, 3)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 90, 90, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 88, 88, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 86, 86, 64)        18496     \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 473344)            0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               60588160  \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 60,608,197\n",
            "Trainable params: 60,608,197\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "92/92 [==============================] - 9s 93ms/step - loss: 3.2392 - accuracy: 0.3719 - val_loss: 1.3046 - val_accuracy: 0.4891\n",
            "Epoch 2/10\n",
            " 2/92 [..............................] - ETA: 7s - loss: 1.1296 - accuracy: 0.6250"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f0ddffde7a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92/92 [==============================] - 8s 90ms/step - loss: 1.1091 - accuracy: 0.5886 - val_loss: 1.1714 - val_accuracy: 0.5627\n",
            "Epoch 3/10\n",
            "11/92 [==>...........................] - ETA: 7s - loss: 0.7252 - accuracy: 0.7614"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-081191f5fdbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd4vhLbNlF-2"
      },
      "source": [
        "Qu'observe-ton au niveau de la perte et de la précision ?\n",
        "<em>À COMPLÉTER</em>\n",
        "\n",
        "Passons donc à l'augmentation de données. La couche correspondante vous est fournie ci-dessous. Cette couche parcourt le jeu de données d’entrainement et applique des transformations sur certaines images choisies aléatoirement pour augmenter le jeu de données. On peut imaginer plusieurs types de modifications, en l'occurrence les transformations qu'on va appliquer à chaque image selon une certaine probabilité sont un inversement horizontale de l’image suivi d’une rotation de 18 degrés pour finir avec un zoom vertical de 10%. Complétez-la puis exécutez-la."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp5zKrUclF-2"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal\"), \n",
        "    layers.experimental.preprocessing.RandomRotation(factor = (-0.1, 0.1)),\n",
        "    layers.experimental.preprocessing.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2))\n",
        "  ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ohpBo8lF-2"
      },
      "source": [
        "Rajoutez ensuite l'augmentation des données à votre CNN et vérifiez les courbes obtenues après entrainement et évaluation du modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdLrgoK-lF-2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d5308f9-ea90-4356-bb96-9144297c3a06"
      },
      "source": [
        "# Le modèle\n",
        "complete_model = data_augmentation\n",
        "complete_model.add(model_with_dropout)\n",
        "\n",
        "complete_model.compile(optimizer =  'adam',\n",
        "              loss =  'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "#complete_model.summary()\n",
        "\n",
        "history = complete_model.fit(\n",
        "    train_set,\n",
        "    batch_size = batch_s,\n",
        "    epochs = 30,\n",
        "    validation_data = test_set\n",
        ")\n",
        "# Compilation du modèle\n",
        "#A COMPLETER\n",
        "# Résumé du modèle\n",
        "#A COMPLETER\n",
        "# Enrainement du modèle\n",
        "#A COMPLETER\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "92/92 [==============================] - 39s 100ms/step - loss: 0.7458 - accuracy: 0.7210 - val_loss: 0.8361 - val_accuracy: 0.6962\n",
            "Epoch 2/30\n",
            "92/92 [==============================] - 9s 98ms/step - loss: 0.7073 - accuracy: 0.7408 - val_loss: 0.8476 - val_accuracy: 0.6689\n",
            "Epoch 3/30\n",
            "92/92 [==============================] - 9s 98ms/step - loss: 0.7160 - accuracy: 0.7374 - val_loss: 0.8841 - val_accuracy: 0.6826\n",
            "Epoch 4/30\n",
            "92/92 [==============================] - 9s 97ms/step - loss: 0.6727 - accuracy: 0.7466 - val_loss: 0.8722 - val_accuracy: 0.6703\n",
            "Epoch 5/30\n",
            "92/92 [==============================] - 9s 98ms/step - loss: 0.6757 - accuracy: 0.7500 - val_loss: 0.9501 - val_accuracy: 0.6703\n",
            "Epoch 6/30\n",
            "92/92 [==============================] - 9s 98ms/step - loss: 0.6865 - accuracy: 0.7394 - val_loss: 0.8257 - val_accuracy: 0.7221\n",
            "Epoch 7/30\n",
            "92/92 [==============================] - 9s 98ms/step - loss: 0.6385 - accuracy: 0.7745 - val_loss: 0.8078 - val_accuracy: 0.7221\n",
            "Epoch 8/30\n",
            "92/92 [==============================] - 9s 98ms/step - loss: 0.6179 - accuracy: 0.7694 - val_loss: 0.8751 - val_accuracy: 0.7057\n",
            "Epoch 9/30\n",
            "92/92 [==============================] - 9s 97ms/step - loss: 0.6198 - accuracy: 0.7653 - val_loss: 0.8032 - val_accuracy: 0.7166\n",
            "Epoch 10/30\n",
            "92/92 [==============================] - 9s 97ms/step - loss: 0.5731 - accuracy: 0.7899 - val_loss: 0.8744 - val_accuracy: 0.7112\n",
            "Epoch 11/30\n",
            "92/92 [==============================] - 9s 97ms/step - loss: 0.5992 - accuracy: 0.7732 - val_loss: 0.8134 - val_accuracy: 0.7193\n",
            "Epoch 12/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.5755 - accuracy: 0.7858 - val_loss: 0.8254 - val_accuracy: 0.7003\n",
            "Epoch 13/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.5833 - accuracy: 0.7864 - val_loss: 0.8201 - val_accuracy: 0.7234\n",
            "Epoch 14/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.5863 - accuracy: 0.7899 - val_loss: 0.7920 - val_accuracy: 0.7357\n",
            "Epoch 15/30\n",
            "92/92 [==============================] - 9s 97ms/step - loss: 0.5464 - accuracy: 0.8011 - val_loss: 0.8089 - val_accuracy: 0.7302\n",
            "Epoch 16/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.5304 - accuracy: 0.8011 - val_loss: 0.8460 - val_accuracy: 0.7302\n",
            "Epoch 17/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.5207 - accuracy: 0.7984 - val_loss: 0.8823 - val_accuracy: 0.7302\n",
            "Epoch 18/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.5300 - accuracy: 0.8045 - val_loss: 0.8317 - val_accuracy: 0.7221\n",
            "Epoch 19/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.5193 - accuracy: 0.8062 - val_loss: 0.8677 - val_accuracy: 0.6798\n",
            "Epoch 20/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.5299 - accuracy: 0.8089 - val_loss: 0.8201 - val_accuracy: 0.7193\n",
            "Epoch 21/30\n",
            "92/92 [==============================] - 9s 97ms/step - loss: 0.5012 - accuracy: 0.8096 - val_loss: 0.8115 - val_accuracy: 0.7384\n",
            "Epoch 22/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.4902 - accuracy: 0.8246 - val_loss: 0.7838 - val_accuracy: 0.7343\n",
            "Epoch 23/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.4868 - accuracy: 0.8290 - val_loss: 0.8335 - val_accuracy: 0.7207\n",
            "Epoch 24/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.4951 - accuracy: 0.8239 - val_loss: 0.8566 - val_accuracy: 0.7098\n",
            "Epoch 25/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.4752 - accuracy: 0.8324 - val_loss: 0.8869 - val_accuracy: 0.7262\n",
            "Epoch 26/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.4708 - accuracy: 0.8256 - val_loss: 0.8393 - val_accuracy: 0.7411\n",
            "Epoch 27/30\n",
            "92/92 [==============================] - 9s 97ms/step - loss: 0.4612 - accuracy: 0.8334 - val_loss: 0.8072 - val_accuracy: 0.7343\n",
            "Epoch 28/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.4243 - accuracy: 0.8460 - val_loss: 0.8692 - val_accuracy: 0.7248\n",
            "Epoch 29/30\n",
            "92/92 [==============================] - 9s 97ms/step - loss: 0.4449 - accuracy: 0.8464 - val_loss: 0.7760 - val_accuracy: 0.7520\n",
            "Epoch 30/30\n",
            "92/92 [==============================] - 9s 96ms/step - loss: 0.4543 - accuracy: 0.8406 - val_loss: 0.7898 - val_accuracy: 0.7343\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-620c9b5cc1dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lower right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (30,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9aq_axslF-2"
      },
      "source": [
        "Nous observons cette fois une nette amélioration des performances, la précision dans le jeu de test atteint une valeur proche de la précision dans le jeu d’entrainement, et la perte du jeu de test baisse de façon quasi continuelle au cours de l’entrainement. Voilà une bonne nouvelle. Que remarquez-vous d'autre ?\n",
        "<em>À COMPLÉTER</em>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShlQm4stlF-3"
      },
      "source": [
        "# 6. Conclusion\n",
        "\n",
        "Bravo, Vous venez de réaliser votre premier CNN ! Vous avez vu comment implémenter, entrainer et évaluer un CNN ainsi qu’améliorer ses performances grâce à la régularisation. Félicitations !\n",
        "\n",
        "Mais il vous reste encore beaucoup à apprendre. Tout d'abord, la précision qu'on obtient n’est que de 73% (celle-ci peut être très proche des 100% pour des jeux de données comme MNIST). Ceci n’est pas étonnant vue la taille de votre réseau qui est très petite (le nombre de paramètres des RNs de Google, par exemple, peut atteindre le million voire le milliard).\n",
        "\n",
        "Évidemment, augmenter la taille du réseau va nettement impacter la rapidité de l'apprentissage. Dans ce cas, l'usage de GPU performants peut améliorer vos performances de manière drastiques, ceux-ci sont très efficaces pour exécuter des  taches de vision. TensorFlow le fait par défaut, mais il est parfois nécessaire d'adapter son fonctionnement, comme par exemple sélectionner le bon GPU (notamment si vous avez un CPU avec un GPU intégré, en général peu performant, et un GPU externe plus puissant). Vous devriez pouvoir trouverfacilement des [ressources abordant le sujet](https://stackoverflow.com/questions/53065420/by-default-does-tensorflow-use-gpu-cpu-simultaneously-for-computing-or-gpu-only).\n",
        "\n",
        "Il y aurait surement d'autres approches pour améliorer les résultats. Il exister des arhitectures plus évoluées de CNNs, comme par exemple ResNet, qui connecte des couches profondes avec des couches mois profondes.\n",
        "\n",
        "Et puis, ici on n'a abordé qu’une seule tache de vision par ordinateur, la classification. Or, c'est la plus simple, et celle-ci est rarement utile à elle seule dans un contexte applicatif (comme pour les voitures autonomes, par exemple). Il y a beaucoup d’autres taches de vision par ordinateur toutes plus difficiles les unes que les autres, comme la détection d’objets (mettre des boites et des labels autour des objets), la segmentation d’image (classifier chaque pixel), la reconnaissance faciale, la restauration d’images etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VgIWs09lF-3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}