{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Livrable 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maxime-Jasserand/ProjetIA/blob/main/Livrable_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZkJXUlQiE4h"
      },
      "source": [
        "# Livrable 1 projet Leyenda\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHej6PGyiENd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Activation\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UUBTqjMltSA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Variables d'environnement\n",
        "\n",
        "Ici nous allons déclarer toutes nos variables globales, par exemple le nombre d'epochs, la taille des images..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYNrHMqtBoo"
      },
      "source": [
        "#Création des paramètres\n",
        "image_scale = 255  # la taille des images\n",
        "\n",
        "# le nombre de canaux de couleurs (1: pour les images noir et blanc; 3 pour les images en couleurs (rouge vert bleu) )\n",
        "image_channels = 3\n",
        "\n",
        "fit_batch_size = 32\n",
        "\n",
        "image_shape = (image_scale, image_scale, image_channels)\n",
        "\n",
        "fit_epochs = 30\n",
        "\n",
        "#folder = \"C:/Windows/System32/\"\n",
        "folder = \"\"\n",
        "\n",
        "modelsPath = \"Model.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve2ux2Yhiqt2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Chargement des données\n",
        "\n",
        "Nous allons commencer par charger les images dans différents datasets.<br>\n",
        "Nous allons diviser chaque dataset en 2 ensembles, un pour l'entrainement et un pour le test.<br>\n",
        "Nous obtiendrons un ensemble de validation en utilisant validation_split lors de l'entraînement.<br>\n",
        "Nous utiliserons une proportion de 80/20.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMo5mm6Vnse2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28dbb08c-14a3-4582-d5c2-ce8bcc2be9bb"
      },
      "source": [
        "#import des datasets\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRFthAt_tx2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "850c9a9c-fe63-4881-94dd-3613feed147e"
      },
      "source": [
        "#unzip datasets ~80s pour finir\n",
        "os.mkdir('/content/data_projet_IA/')\n",
        "!unzip \"/content/gdrive/MyDrive/IA/Dataset Livrable 1 - Photo.zip\" -d \"/content/data_projet_IA/initialDatasets\" > /dev/null\n",
        "print(\"Photos copiés\")\n",
        "!unzip \"/content/gdrive/MyDrive/IA/Dataset Livrable 1 - Text.zip\" -d \"/content/data_projet_IA/initialDatasets\" > /dev/null\n",
        "print(\"Textes copiés\")\n",
        "!unzip \"/content/gdrive/MyDrive/IA/Dataset Livrable 1 - Schematics.zip\" -d \"/content/data_projet_IA/initialDatasets\" > /dev/null\n",
        "print(\"Schémas copiés\")\n",
        "!unzip \"/content/gdrive/MyDrive/IA/Dataset Livrable 1 - Sketch.zip\" -d \"/content/data_projet_IA/initialDatasets\" > /dev/null\n",
        "print(\"Esquisses copiés\")\n",
        "!unzip \"/content/gdrive/MyDrive/IA/Dataset Livrable 1 - Painting.zip\" -d \"/content/data_projet_IA/initialDatasets\" > /dev/null\n",
        "print(\"Peintures copiés\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Photos copiés\n",
            "Textes copiés\n",
            "Schémas copiés\n",
            "Esquisses copiés\n",
            "Peintures copiés\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YkarF8DEiCWm",
        "outputId": "f9c41e61-8739-4952-9537-55c37563b706"
      },
      "source": [
        "#[CREATION TRAIN SET] ~120s pour finir\n",
        "\n",
        "#copier les fichiers vers le train set\n",
        "shutil.copytree(folder + '/content/data_projet_IA/initialDatasets', \n",
        "                folder + '/content/data_projet_IA/train_set', \n",
        "                symlinks=False, ignore=None,\n",
        "                ignore_dangling_symlinks=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/data_projet_IA/train_set'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1kn981SuAks"
      },
      "source": [
        "#récupérer la taille des dossiers\n",
        "path, dirs, files = next(os.walk(folder + '/content/data_projet_IA/train_set/Photo'))\n",
        "path2, dirs2, files2 = next(os.walk(folder + '/content/data_projet_IA/train_set/Sketch'))\n",
        "\n",
        "#move 20% images to validation\n",
        "os.mkdir(folder + '/content/data_projet_IA/validation_set/')\n",
        "os.mkdir(folder + '/content/data_projet_IA/validation_set/Photo/')\n",
        "os.mkdir(folder + '/content/data_projet_IA/validation_set/Text/')\n",
        "os.mkdir(folder + '/content/data_projet_IA/validation_set/Schematics/')\n",
        "os.mkdir(folder + '/content/data_projet_IA/validation_set/Sketch/')\n",
        "os.mkdir(folder + '/content/data_projet_IA/validation_set/Painting/')\n",
        "\n",
        "photoList = os.listdir(folder + '/content/data_projet_IA/train_set/Photo')\n",
        "textList = os.listdir(folder + '/content/data_projet_IA/train_set/Text')\n",
        "schematicsList = os.listdir(folder + '/content/data_projet_IA/train_set/Schematics')\n",
        "sketchList = os.listdir(folder + '/content/data_projet_IA/train_set/Sketch')\n",
        "paintingList = os.listdir(folder + '/content/data_projet_IA/train_set/Painting')\n",
        "\n",
        "for i in range(0,int(len(files)/5)):\n",
        "  shutil.move(folder + \"/content/data_projet_IA/train_set/Photo/\" + photoList[i], folder + \"/content/data_projet_IA/validation_set/Photo/\" + photoList[i])\n",
        "  shutil.move(folder + \"/content/data_projet_IA/train_set/Text/\" + textList[i], folder + \"/content/data_projet_IA/validation_set/Text/\" + textList[i])\n",
        "  shutil.move(folder + \"/content/data_projet_IA/train_set/Schematics/\" + schematicsList[i], folder + \"/content/data_projet_IA/validation_set/Schematics/\" + schematicsList[i])\n",
        "  shutil.move(folder + \"/content/data_projet_IA/train_set/Painting/\" + paintingList[i], folder + \"/content/data_projet_IA/validation_set/Painting/\" + paintingList[i])\n",
        "for i in range(0,int(len(files2)/5)):\n",
        "  shutil.move(folder + \"/content/data_projet_IA/train_set/Sketch/\" + sketchList[i], folder + \"/content/data_projet_IA/validation_set/Sketch/\" + sketchList[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF9wWwaZltbp"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Traitement des données\n",
        "\n",
        "Pour pouvoir exploiter nos données nous allons devoir leur appliquer un certain nombre de pré-traitements :\n",
        "*  Elles devront toutes avoir la même taille pour pouvoir être exploitée par le CNN.\n",
        "\n",
        "*  Afin de réduire l'overfitting nous allons effectuer une Data Augmentation.\n",
        "\n",
        "*  Nous allons garder un set sans Data Augmentation pour comparer les résultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yo0RTMgvIjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7489a63-ba43-4a29-e5bb-3a4c15b5e216"
      },
      "source": [
        "# training_data_generator: charge les données d'entrainement en mémoire\n",
        "# quand il charge les images, il les ajuste (change la taille, les dimensions, la direction ...)\n",
        "# aléatoirement afin de rendre le modèle plus robuste à la position du sujet dans les images\n",
        "# Note: On peut utiliser cette méthode pour augmenter le nombre d'images d'entrainement (data augmentation)\n",
        "training_data_generator = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "        zoom_range=0.15,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.1,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# validation_data_generator: charge les données de validation en memoire\n",
        "validation_data_generator = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "# training_generator: indique la méthode de chargement des données d'entrainement\n",
        "training_generator = training_data_generator.flow_from_directory(\n",
        "    folder + '/content/data_projet_IA/train_set',  # Place des images d'entrainement\n",
        "    color_mode='rgb',  # couleur des images\n",
        "    target_size=(image_scale, image_scale),  # taille des images\n",
        "    batch_size=fit_batch_size,  # nombre d'images à entrainer (batch size)\n",
        "    class_mode=\"categorical\",  # classement binaire (problème de 2 classes)\n",
        "    shuffle=True)  # on \"brasse\" (shuffle) les données -> pour prévenir le surapprentissage\n",
        "\n",
        "# validation_generator: indique la méthode de chargement des données de validation\n",
        "validation_generator = validation_data_generator.flow_from_directory(\n",
        "    folder + '/content/data_projet_IA/validation_set',  # Place des images de validation\n",
        "    color_mode='rgb',  # couleur des images\n",
        "    target_size=(image_scale, image_scale),  # taille des images\n",
        "    batch_size=fit_batch_size,  # nombre d'images à valid\n",
        "    class_mode=\"categorical\",  # classement binaire (problème de 2 classes)\n",
        "    shuffle=True)  # on \"brasse\" (shuffle) les données -> pour prévenir le surapprentissage\n",
        "\n",
        "# On imprime l'indice de chaque classe (Keras numerote les classes selon l'ordre des dossiers des classes)\n",
        "# Dans ce cas => [2: 0 et 7:1]\n",
        "print(training_generator.class_indices)\n",
        "print(validation_generator.class_indices)\n",
        "\n",
        "# On charge les données d'entrainement et de validation\n",
        "# x_train: Les données d'entrainement\n",
        "# y_train: Les Ètiquettes des données d'entrainement\n",
        "# x_val: Les données de validation\n",
        "# y_val: Les Ètiquettes des données de validation\n",
        "(x_train, y_train) = training_generator.next()\n",
        "(x_val, y_val) = validation_generator.next()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 33125 images belonging to 5 classes.\n",
            "Found 8281 images belonging to 5 classes.\n",
            "{'Painting': 0, 'Photo': 1, 'Schematics': 2, 'Sketch': 3, 'Text': 4}\n",
            "{'Painting': 0, 'Photo': 1, 'Schematics': 2, 'Sketch': 3, 'Text': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUN3PBEXltfB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Déclaration des modèles\n",
        "\n",
        "Ensuite nos allons construire nos différents modèles.<br>\n",
        "Nous avons choisi d'utiliser 3 modèles :\n",
        "\n",
        "\n",
        "*   Un premier modèle, très simple, composé de 2 couches de convolutions et 2 couches denses.\n",
        "*   Un modèle avec 2 fois plus de couches de convolution.\n",
        "*   Un modèle avec une architecture reconnue, VGG-16.\n",
        "\n",
        "L'idée est de pouvoir comparer différents modèles afin de visualiser en quoi la profondeur va influencer la précision et le temps d'apprentissage.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7wkH153ONkn"
      },
      "source": [
        "def feature_extraction(input):\n",
        "\n",
        "    # 1-couche de convolution avec nombre de filtre  (exp 32)  avec la taille de la fenetre de ballaiage exp : 3x3\n",
        "    # 2-fonction d'activation exp: sigmoid, relu, tanh ...\n",
        "    # 3-couche d'echantillonage (pooling) pour reduire la taille avec la taille de la fenetre de ballaiage exp :2x2\n",
        "\n",
        "    # **** On répète ces étapes tant que nécessaire ****\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(input)\n",
        "    x = Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(x)\n",
        "    x = Dense(128, activation='relu')\n",
        "    x = Dense(128, activation='relu')\n",
        "\n",
        "    return x\n",
        "\n",
        "def fully_connected(encoded):\n",
        "# Flatten: pour convertir les matrices en vecteurs pour la couche MLP\n",
        "# Dense: une couche neuronale simple avec le nombre de neurone (exemple 64)\n",
        "# fonction d'activation exp: sigmoid, relu, tanh ...\n",
        "  x = Flatten(input_shape=(255,255,3))(encoded)\n",
        "\n",
        "# Puisque'on a une classification binaire, la dernière couche doit être formée d'un seul neurone avec une fonction d'activation sigmoide\n",
        "# La fonction sigmoide nous donne une valeur entre 0 et 1\n",
        "# On considère les résultats <=0.5 comme l'image appartenant à la classe 0 (c.-à-d. la classe qui correspond au chiffre 2)\n",
        "# on considère les résultats >0.5 comme l'image appartenant à la classe 0 (c.-à-d. la classe qui correspond au chiffre 7)\n",
        "  x = Dense(5)(x)\n",
        "  sortie = Activation('softmax')(x)\n",
        "  return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFl8uAhnvOhF"
      },
      "source": [
        "\"\"\"model = tf.keras.applications.VGG16(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_tensor=None,\n",
        "    input_shape=None,\n",
        "    pooling=None,\n",
        "    classes=5,\n",
        "    classifier_activation=\"softmax\",\n",
        ")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4d_FD5zltiI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 5. Entraînement\n",
        "\n",
        "Il est temps d'entraîner nos modèles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVIquXOVj1IY"
      },
      "source": [
        "#compilation du modèle \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"RMSProp\", metrics=['accuracy'])\n",
        "\n",
        "modelcheckpoint = ModelCheckpoint(filepath=modelsPath,\n",
        "                                  monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "early = EarlyStopping(monitor='val_accuracy', min_delta=0.005, patience=10, verbose=1, mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "sIgGjcUpUT1M",
        "outputId": "33eeae2c-dfe1-45b2-d6c6-16dcca7af81d"
      },
      "source": [
        "visualkeras.layered_view(model, to_file='output.png', legend = True).show() # write and show model architecture"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c14e4f439165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayered_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# write and show model architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'visualkeras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "u0cXVgg-kVIP",
        "outputId": "c387290c-419d-403a-893e-821c674677da"
      },
      "source": [
        "#entrainement du modèle \n",
        "classifier = model.fit(training_generator,\n",
        "                       epochs=10,  # nombre d'époques\n",
        "                       batch_size=fit_batch_size,  # nombre d'images entrainées ensemble\n",
        "                       validation_data=validation_generator,  # données de validation\n",
        "                       verbose=1,  # mets cette valeur ‡ 0, si vous voulez ne pas afficher les détails d'entrainement\n",
        "                       shuffle=True,\n",
        "                       max_queue_size=15, \n",
        "                       # les fonctions à appeler à la fin de chaque époque (dans ce cas modelcheckpoint: qui sauvegarde le modèle)\n",
        "                       callbacks=[modelcheckpoint, early],\n",
        "                       workers = 4)  # shuffle les images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e5ed9e89fc59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#entrainement du modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m classifier = model.fit(training_generator,\n\u001b[0m\u001b[1;32m      3\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# nombre d'époques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_batch_size\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# nombre d'images entrainées ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# données de validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_24xs6bqltk7"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 6. Tester les modèles\n",
        "\n",
        "Nous allons appliquer nos modèles sur les données de test afin de nous assurer de leur précision.<br>\n",
        "Nous allons également afficher les matrices de confusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sc-CN97mHbA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 7. Influence des hyperparamètres\n",
        "\n",
        "Pour tenter d'améliorer nos modèles nous allons effectuer de l'hypertuning : en faisant varier certains hyperparamètres nous allons peut-être obtenir des modèles plus rapides ou plus précis.\n",
        "Nous ne cherchons pas à trouver le modèle le plus précis mais simplement a visualiser l'influence des hyperparamètres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8jReVRUmGDK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 8. Transfert learning\n",
        "\n",
        "Dans l'optique d'obtenir le modèle le plus précis possible nous allons nous servir d'un modèle pré-entraîné.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ8YXqSEmHgr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 9. Comparer les résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkaLCTg9mHki"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 10. Conclusion"
      ]
    }
  ]
}